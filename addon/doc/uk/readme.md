# Описувач ввмісту за допомогою ШІ для NVDA

Цей додаток дозволяє отримати докладні описи для зображень та іншого візуально недоступного вмісту.

Використовуючи мультимодальні можливості великої мовної моделі GPT-4, ми прагнемо забезпечити найкращі у своєму класі описи вмісту. Для отримання додаткової інформації про базову модель зверніться до [GPT-4V](https://openai.com/research/gpt-4v-system-card).

## Особливості

* Опис об'єкта у фокусі, об'єкта навігатора, всього екрана або фотографії з вебкамери
* Опис будь-якого зображення, яке було скопійовано до буфера обміну — чи то зображення з електронного листа, чи шлях у провіднику Windows
* Визначає, чи обличчя користувача розташоване в центрі кадру, використовуючи алгоритми комп'ютерного зору (не потребує доступу до платного API).
* Підтримує кілька постачальників (GPT4 OpenAI, Gemini Google, Claude 3 Anthropic і llama.cpp)
* Підтримує широкий спектр форматів, включаючи PNG (.png), JPEG (.jpeg і .jpg), WEBP (.webp) і неанімований GIF (.gif)
* Додатково кешує відповіді, щоб зберегти ліміт API
* Для розширеного використання налаштуйте підказку та кількість токенів, щоб адаптувати інформацію до ваших потреб
* Відтворення Markdown для легкого доступу до структурованої інформації (просто вставте, наприклад, «відповідай в Markdown» у кінці підказки)

## Варіант використання

За цим проектом стояло кілька основних мотивів.

NVDA початково здатна виконувати оптичне розпізнавання символів (OCR), що змінює правила гри. Якщо ви намагаєтеся витягти текст із зображення або PDF-документа, це те, що вам потрібно.

Однак OCR здатний аналізувати лише ті дані, які «можуть» бути текстом. Він не здатен врахувати контекст, об'єкти та взаємозв'язки, передані в цих зображеннях. А в інтернеті їх повно. Логотипи, портрети, меми, іконки, графіки, діаграми, гістограми... Все, що завгодно. Вони всюди, і, як правило, не в тому форматі, який користувачі екранних читачів можуть інтерпретувати.
Донедавна існувала непохитна довіра до авторів вмісту, які надавали альтернативні текстові описи. Хоча це все ще є обов'язковим, важко змінити той факт, що високий стандарт якості є скоріше винятком, ніж правилом.

Тепер можливості майже безмежні. Ви можете:

* Візуалізувати робочий стіл або конкретне вікно, щоб зрозуміти розміщення іконок під час навчання інших
* Отримати детальну інформацію про стан ігор, віртуальних машин тощо, коли звук недостатній або недоступний
* Зрозуміти, що відображається на графіку
* Розпізнати знімки екрана
* Переконатись, що ваше обличчя чітко дивиться на камеру, перш ніж записувати відео або брати участь в онлайн-зустрічі

## Моделі

* [GPT4 vision](https://platform.openai.com/docs/guides/vision)
* [Google Gemini pro vision](https://blog.google/technology/ai/google-gemini-ai/)
* [Claude 3 (Haiku, Sonett, and Opus)](https://docs.anthropic.com/claude/docs/vision)
* [llama.cpp (надзвичайно нестабільний і повільний залежно від вашого апаратного забезпечення, перевірено на роботу з моделями llava-v1.5/1.6, BakLLaVA, Obsidian і MobileVLM 1.7B/3B)](https://github.com/ggerganov /llama.cpp)

Дотримуйтеся наведених нижче інструкцій, щоб кожна з них працювала.

## Початок роботи

Завантажте останню версію додатка за [цим посиланням](https://github.com/cartertemm/AI-content-describer/releases/latest/). Клацніть файл на комп’ютері з інстальованою NVDA, а потім виконайте наведені нижче інструкції, щоб отримати ключ API від підтримуваного постачальника:
Якщо ви не впевнені, якого постачальника  використовувати, розробники та тестувальники цього додатка погоджуються, що Gemini наразі пропонує більш прийнятну ціну, тоді як OpenAI, схоже, забезпечує вищий ступінь точності. Claude 3 haiku є найдешевшим і найшвидшим варіантом, але якість його не дуже висока.
Звісно, ці результати суттєво залежать від поставленого завдання, тому ми рекомендуємо експериментувати з різними моделями і підказками, щоб знайти найкращий варіант.

### Отримання ключа API від OpenAI:

1. Перейдіть на сторінку [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
2. Якщо у вас ще немає облікового запису, створіть його. Якщо маєте, увійдіть.
3. На сторінці ключів API натисніть (Створити новий секретний ключ). Скопіюйте його в буфер обміну.
4. Поповніть рахунок принаймні на 1 дол
5. У діалозі налаштувань NVDA прокрутіть вниз до категорії «Описувач вмісту за допомогою ШІ», потім перейдіть до поля Ключ API і вставте туди щойно згенерований ключ.

На момент написання цієї статті OpenAI видає кредити на нові акаунти розробників, які можна використовувати протягом трьох місяців, після чого вони втрачаються. Після цього періоду вам потрібно буде придбати кредити. Звичайне використання не повинно перевищувати $5.00 на місяць. Для порівняння, оригінальна версія цього додатка була розроблена за трохи менше долара. Ви завжди можете увійти до свого облікового запису OpenAI і натиснути на "використання", щоб дізнатись свій ліміт.

### Отримання ключа API від Google

1. Спочатку вам потрібно створити проект у робочому просторі Google, перейшовши за посиланням нижче. Переконайтеся, що ви увійшли до свого облікового запису. [https://console.cloud.google.com/projectcreate](https://console.cloud.google.com/projectcreate)
2. Створіть назву від чотирьох до тридцяти символів, як-от «gemini» або «NVDA add-on»
3. Перейдіть за цією URL-адресою: [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)
4. Натисніть «створити ключ API»
5. У  діалозі налаштувань NVDA прокрутіть вниз до категорії Описувач вмісту за допомогою ШІ, потім виберіть "керувати моделями (alt+m)", виберіть "Google Gemini" як постачальника, перейдіть до поля "Ключ API" і вставте сюди щойно згенерований ключ.

### Отримання ключа API від Anthropic

1. Увійдіть до [Anthropic console](https://console.anthropic.com/login).
2. Натисніть на свій профіль -> Ключі API.
3. Натисніть Створити ключ.
4. Введіть назву для ключа, наприклад "AIContentDescriber", потім натисніть "Створити ключ" і скопіюйте значення, що з'явиться. Це те, що ви вставите у поле ключа API під категорією Описувач вмісту за допомогою ШІ діалогу налаштувань NVDA -> керувати моделями -> Клод 3.
5. Якщо ви ще цього не зробили, придбайте кредитів щонайменше на 5 доларів США на сторінці планів за адресою [https://console.anthropic.com/settings/plans](https://console.anthropic.com/settings/plans).

### Налаштування llama.cpp

Наразі цей провайдер є дещо нестабільним, і ваш досвід може значно варіюватися. Насправді його варто використовувати лише досвідченим користувачам, які зацікавлені у використанні локальних саморозміщених моделей і мають для цього відповідне апаратне забезпечення.

1. Завантажте llama.cpp. На момент написання цієї статті цей [pull request](https://github.com/ggerganov/llama.cpp/pull/5882) вилучає мультимодальні можливості, тому вам краще використовувати [останню версію з підтримкою цього](https://github.com/ggerganov/llama.cpp/releases/tag/b2356).
Якщо ви працюєте на графічному адаптері Nvidia з підтримкою CUDA, завантажте ці готові двійкові файли:
[llama-b2356-bin-win-cublas-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/llama-b2356-bin-win-cublas-cu12.2.0-x64.zip) та [cudart-llama-bin-win-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/cudart-llama-bin-win-cu12.2.0-x64.zip)
Кроки для роботи з іншим графічним адаптером винесено за рамки цієї статті, але їх можна знайти у файлі readme llama.cpp.
2. Розпакуйте обидва файли до однієї теки.
3. Знайдіть у Huggingface квантовані формати моделей, які ви хочете використовувати. Для LLaVA 1.6 Vicuna 7B: [llava-v1.6-vicuna-7b.Q4_K_M.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/llava-v1.6-vicuna-7b.Q4_K_M.gguf) та [mmproj-model-f16.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/mmproj-model-f16.gguf)
4. Покладіть ці файли до теки з рештою двійкових файлів llama.cpp.
5. З командного рядка запустіть двійковий файл сервера llava.cpp, передавши файли .gguf для моделі та мультимодального проектора (як показано нижче):
`server.exe -m llava-v1.6-vicuna-7b.Q4_K_M.gguf --mmproj mmproj-model-f16.gguf`.
6. У  діалозі налаштувань NVDA прокрутіть вниз до категорії Описувач вмісту за допомогою ШІ, потім виберіть "керувати моделями (alt+m)", виберіть "llama.cpp" як постачальника, перейдіть до поля базової URL-адреси і введіть кінцеву точку, показану в консолі (початково "http://localhost:8080").
7. Крім того, ви можете пропустити деякі з цих кроків і запустити llama.cpp на віддаленому сервері з вищими характеристиками, ніж на вашій локальній машині, а потім ввести ту саму кінцеву точку.

## Використання

Початково призначено чотири гарячі клавіші:

* NVDA+shift+i: відкриває спливаюче меню із запитом, чи потрібно описати поточний фокус, об’єкт навігатора чи весь екран за допомогою ШІ.
* NVDA+shift+u: Описати вміст поточного об'єкта навігатора за допомогою ШІ.
* NVDA+shift+y: Описати зображення (або шлях до файлу зображення) у буфері обміну за допомогою ШІ.
* NVDA+shift+j: описати положення вашого обличчя в кадрі вибраної камери. Якщо у вас під’єднано кілька камер, перейдіть до меню описувача вмісту за допомогою ШІ (NVDA+shift+i) і виберіть ту, яку ви бажаєте використати, за допомогою пункту «Виберіть камеру» у підменю визначення обличчя.

Три жести не призначені:

* Описати вміст об'єкта, на якому перебуває фокус, використовуючи ШІ.
* Зробити знімок екрана, а потім описати його за допомогою ШІ.
* Зробити знімок за допомогою вибраної камери, а потім описати його за допомогою ШІ.

Ви можете налаштувати їх у будь-який час за допомогою діалогу «Жести вводу».

## Внески

Усі вони високо оцінені та будуть зараховані.
Над додатком працювали наступні люди.

* [Mazen](https://github.com/mzanm)
* [Костенков-2021](https://github.com/Kostenkov-2021)
* [Nidza07](https://github.com/nidza07)
* [Георгій Галас](nvda.translation.uk@gmail.com)
* [Умут Коркмаз](umutkork@gmail.com)
* [Platinum_Hikari](urbain_onces.0r@icloud.com)

Виникла проблема? Надішліть її в [систему відстеження проблем](https://github.com/cartertemm/AI-content-describer/issues)

Маєте пропозицію щодо нової функції? Створіть для цього також обговорення, і ми зможемо обговорити його реалізацію. Запити без супутніх проблем будуть розглянуті, але, ймовірно, займуть більше часу для всіх, особливо якщо я вирішу, що нове виправлення або функціонал повинен працювати інакше, ніж було запропоновано.

Якщо у вас немає Github або ви не бажаєте ним користуватися, ви можете [написати мені листа](mailto:cartertemm@gmail.com) - cartertemm@gmail.com

Дякую за підтримку!
