# Описатель контента с помощью искусственного интеллекта (ИИ)для NVDA

Это дополнение позволяет получать подробные описания для изображений и другого визуально недоступного контента.

Используя многообразные возможности передовых моделей искусственного интеллекта (далее - ИИ) и алгоритмов компьютерного зрения, мы стремимся предоставить лучшие в своём классе описания контента и повысить общую независимость. Более подробную информацию о моделях, лежащих в основе дополнения, можно найти в соответствующем разделе этого документа.

## Функции.


* Описывает объект фокуса, объект навигатора, весь экран и позволяет сделать снимок с камеры.
* Описывает любое изображение, которое было скопировано в буфер обмена, будь то картинка из электронной почты или путь к изображению в проводнике windows
* Подсказывает, расположено ли лицо пользователя в центре кадра, используя алгоритмы компьютерного зрения (не требует платного доступа к API)
* Поддерживает множество поставщиков моделей ИИ (OpenAI's GPT4, Google's Gemini, Anthropic's Claude 3, и llama.cpp)
* Поддерживает широкий спектр форматов, включая PNG (.png), JPEG (.jpeg и .jpg), WEBP (.webp) и неанимированный GIF (.gif)
* Опционально кэширует ответы для сохранения квоты API
* Для расширенного использования настраивайте подсказки и количество токенов, чтобы адаптировать информацию к вашим потребностям.
* Рендеринг в формате Markdown для лёгкого доступа к структурированной информации (просто вставьте, например, "ответить в формате Markdown" в конец подсказки)

## Пример использования

У этого проекта было несколько основных мотивов.

NVDA способна выполнять оптическое распознавание символов (OCR) из коробки, что меняет правила игры. Если вы пытаетесь получить текст из изображения или PDF-документа, это то, что вам нужно.

Однако OCR способен анализировать только те данные, которые *могут* быть текстом. Он не в состоянии учесть контекст, объекты и отношения, переданные в этих изображениях. А интернет полон ими. Логотипы, портреты, мемы, иконки, графики, диаграммы, гистограммы и линейные графики... Да что угодно. Они повсюду, и, как правило, не в том формате, в котором их могут интерпретировать пользователи скринридеров.
До недавнего времени авторы контента неукоснительно предоставляли альтернативные текстовые описания. Хотя это по-прежнему необходимо, трудно изменить тот факт, что высокий стандарт качества является исключением, а не правилом.

Теперь возможности практически безграничны. Вы можете:

* Визуализировать рабочий стол или конкретное окно, чтобы понять расположение значков при обучении.
* Получить подробную информацию о состоянии игр, виртуальных машин и т. д., когда звук недостаточен или недоступен.
* Понять, что отображается на графике.
* Демистифицировать скриншоты или общий доступ к экрану в Zoom или Microsoft Teams.
* Убедиться, что ваше лицо чётко смотрит в камеру, а фон имеет профессиональный вид, прежде чем записывать видео или участвовать в онлайн-совещаниях.

## Модели

* [GPT4 vision](https://platform.openai.com/docs/guides/vision)
* [Google Gemini pro vision](https://blog.google/technology/ai/google-gemini-ai/)
* [Claude 3 (Haiku, Sonett и Opus)](https://docs.anthropic.com/claude/docs/vision)
* [llama.cpp (крайне нестабилен и медлителен в зависимости от вашего оборудования, проверен на работу с моделями llava-v1.5/1.6, BakLLaVA, Obsidian и MobileVLM 1.7B/3B)](https://github.com/ggerganov/llama.cpp)

Следуйте инструкциям, приведённым ниже, чтобы заставить каждую из них работать.

## Начало работы

Загрузите последнюю версию дополнения по [этой ссылке](https://github.com/cartertemm/AI-content-describer/releases/latest/). Нажмите на файл на компьютере с установленной NVDA, а затем следуйте приведённым ниже инструкциям, чтобы получить API-ключ от поддерживаемого поставщика.
Если вы не уверены, какой из них использовать, то, по общему мнению разработчиков и тестеровщиков этого дополнения, Gemini в настоящее время предлагает более приемлемые цены, в то время как OpenAI, похоже, обеспечивает более высокую степень точности. Claude 3 haiku - самый дешёвый и быстрый вариант, но его качество оставляет желать лучшего.
Конечно, эти результаты сильно зависят от поставленной задачи, поэтому мы рекомендуем поэкспериментировать с различными моделями и подсказками, чтобы найти то, что работает лучше всего.

### Получение ключа API от OpenAI:

1. Перейдите на [сайт OpenAI](https://platform.openai.com/account/api-keys).
2. Если у вас ещё нет учетной записи, создайте её. Если есть, войдите в систему.
3. На странице  API keys (API-ключи) нажмите create a new secret key (создать новый секретный ключ). Скопируйте его в буфер обмена.
4. Пополните счёт на сумму не менее 1 доллара.
5. В диалоговом окне настроек NVDA выберите "управление моделями (alt+m)", выберите "GPT4 Vision" в качестве поставщика, при помощи клавиши tab перейдите в поле API-ключа и вставьте сюда только что созданный секретный ключ.

На момент написания документации OpenAI выдаёт кредиты новым аккаунтам разработчиков, которые можно использовать в течение трёх месяцев, после чего они сгорают. По истечении этого срока вам придётся покупать кредиты. Обычно их использование не превышает $5,00 в месяц. Для сравнения, оригинальная версия этого дополнения была разработана за сумму чуть меньше доллара. Вы всегда можете войти в свой аккаунт OpenAI и нажать на "usage" ("Использование"), чтобы узнать свою квоту.

### Получение ключа API от Google

1. Сначала вам нужно создать проект рабочего пространства Google, перейдя по [этой ссылке](https://console.cloud.google.com/projectcreate). Убедитесь, что вы вошли в свой аккаунт Google.
2. Создайте имя длиной от четырёх до тридцати символов, например "gemini" или "NVDA add-on".
3. Перейдите по [этой ссылке](https://makersuite.google.com/app/apikey)
4. Нажмите "Создать ключ API".
5. В диалоговом окне настроек NVDA прокрутите вниз до категории AI Content Describer, затем выберите "управление моделями (alt+m)", выберите "Google Gemini" в качестве поставщика, перейдите в поле API-ключа и вставьте сюда только что созданный ключ.

### Получение ключа API от Anthropic

1. Войдите в [консоль Anthropic] (https://console.anthropic.com/login).
2. Нажмите на свой профиль -> API-ключи.
3. Нажмите кнопку Создать ключ.
4. Введите имя ключа, например "AIContentDescriber", затем нажмите "Создать ключ" и скопируйте появившееся значение. Это то, что вы вставите в поле API-ключа в категории Ai Content Describer в диалоге настроек NVDA -> управление моделями -> Клод 3.
5. Если вы ещё не сделали этого, приобретите кредиты на сумму не менее $5 на странице планов по адресу https://console.anthropic.com/settings/plans.

### Настройка llama.cpp

В настоящее время этот поставщик имеет некоторые ошибки, и ваш пробег может быть очень большим. Его могут использовать только опытные пользователи, заинтересованные в запуске локальных самодостаточных моделей и имеющие соответствующее оборудование.

1. Загрузите llama.cpp. На момент написания этой статьи, этот [pull request](https://github.com/ggerganov/llama.cpp/pull/5882) удаляет мультимодальные возможности, поэтому вы захотите использовать [последнюю версию с поддержкой этого](https://github.com/ggerganov/llama.cpp/releases/tag/b2356).
Если вы работаете на графическом адаптере Nvidia с поддержкой CUDA, загрузите эти предварительно собранные двоичные файлы:
[llama-b2356-bin-win-cublas-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/llama-b2356-bin-win-cublas-cu12.2.0-x64.zip) и [cudart-llama-bin-win-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/cudart-llama-bin-win-cu12.2.0-x64.zip).
Шаги по работе с другим графическим адаптером не входят в эту тему, но могут быть найдены в readme к llama.cpp.
2. Распакуйте оба файла в одну папку.
3. Найдите в Huggingface квантованные форматы моделей, которые вы хотите использовать. Для LLaVA 1.6 Vicuna 7B: [llava-v1.6-vicuna-7b.Q4_K_M.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/llava-v1.6-vicuna-7b.Q4_K_M.gguf) и [mmproj-model-f16.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/mmproj-model-f16.gguf).
4. Поместите эти файлы в папку с остальными двоичными файлами llama.cpp.
5. Из командной строки запустите двоичный файл сервера llava.cpp, передав ему файлы .gguf для модели и мультимодального проектора (как показано ниже):
`server.exe -m llava-v1.6-vicuna-7b.Q4_K_M.gguf --mmproj mmproj-model-f16.gguf`.
6. В диалоговом окне настроек NVDA прокрутите вниз до категории AI Content Describer, затем выберите "manage models (alt+m)", выберите "llama.cpp" в качестве провайдера, сделайте закладку в поле base URL и введите конечную точку, показанную в консоли (по умолчанию "http://localhost:8080").
7. В качестве альтернативы вы можете пропустить некоторые из этих шагов и запустить llama.cpp на удаленном сервере с более высокими характеристиками, чем у вашей локальной машины, а затем ввести эту конечную точку.


## Использование

По умолчанию привязаны четыре горячие клавиши:

* NVDA+shift+i: Вызывает меню, в котором предлагается описать с помощью ИИ текущий фокус, объект навигатора, изображение с камеры или весь экран.
* NVDA+shift+u: Описать содержимое текущего объекта навигатора с помощью ИИ.
* NVDA+shift+y: Описать изображение или изображение по пути к файлу  в буфере обмена с помощью ИИ.
* NVDA+shift+j: Описать положение вашего лица в кадре выбранной камеры. Если подключено несколько камер, перейдите в меню описателя содержимого AI (NVDA+shift+i) и выберите ту, которую хотите использовать, с помощью пункта "выбрать камеру" в подменю распознавания лиц.

Три жеста не привязаны:

* Описать содержимое текущего элемента в фокусе с помощью ИИ.
* Сделать снимок экрана, а затем описать его с помощью ИИ.
* Сделать снимок выбранной камерой, а затем описать его с помощью ИИ.

Не стесняйтесь настраивать их в любое время из диалога  жестов ввода.

## Создание дополнения

Для создания пакета дополнения из исходного кода вам потребуется:

* дистрибутив Python (рекомендуется версия 3.7 или более поздняя). Установщики для Windows можно найти на [Python Website](https://www.python.org). Обратите внимание, что в настоящее время для подготовки исходного кода NVDA и включённых в него сторонних модулей требуется 32-битная версия Python 3.7.
* Scons - [Сайт](https://www.scons.org/) - версия 4.3.0 или более поздняя. Вы можете установить его через PIP. `pip install scons`
* Markdown 3.3.0 или более поздней версии. `pip install markdown`.

Затем откройте выбранный вами терминал:

```
git clone https://github.com/cartertemm/AI-content-describer.git
cd AI-content-describer
scons
```

После завершения выполнения команды `scons` в корень репозитория будет помещён файл *.nvda-addon, готовый к тестированию и выпуску.

Если вы добавите дополнительные строки, которые необходимо перевести, важно перестроить файл .pot следующим образом:

```
scons pot
```

## Как перевести?

На компьютере под управлением ОС Windows:

* скачайте [poedit](https://poedit.net/). Это программа, которую вы будете использовать для перевода каждого сообщения с английского.
* скачайте файл .pot со всеми строками [здесь](https://raw.githubusercontent.com/cartertemm/AI-content-describer/main/AIContentDescriber.pot)
* Откройте файл, который вы только что скачали, в программе poedit. В появившемся окне нажмите "Создать новый перевод" и выберите целевой язык.
* Переведите содержимое исходного текста на язык перевода, а затем вставьте его в поле перевода. Для получения дополнительной помощи нажмите элемент списка -> вхождения кода, затем поднимитесь на строку вверх и прочитайте комментарий, начинающийся с "# Translators: " ("# Переводчики: "). Эти комментарии дополнительно доступны в одном месте в файле .pot.
* Когда всё будет готово, нажмите файл -> сохранить или нажмите ctrl+s, затем выберите место, где будут храниться новые файлы .mo и .po. Эти файлы следует отправить мне по электронной почте или прикрепить в запросе на исправление.
* Переведите содержимое readme.md (этот файл). Не забудьте отправить и его!

## Вклад

Вклад высоко ценится и будет отмечен. Никто и ничто не будет забыто.

Над дополнением работали следующие люди.

* [Mazen](https://github.com/mzanm)
* [Kostenkov-2021](https://github.com/Kostenkov-2021)
* [nidza07](https://github.com/nidza07)
* [Heorhii Halas](nvda.translation.uk@gmail.com)

Столкнулись с проблемой? Отправьте её в [issue tracker](https://github.com/cartertemm/AI-content-describer/issues).

Есть предложение по новой функции? Создайте тикет, и мы сможем обсудить его реализацию. Pull-запросы без связанных с ними проблем будут рассмотрены, но, скорее всего, займут больше времени, особенно если я решу, что новое исправление или функциональность должны работать не так, как было предложено.

Переводы приветствуются с распростёртыми объятиями.

Если у вас нет Github или вы предпочитаете не использовать его, вы можете [написать мне письмо](mailto:cartertemm@gmail.com) - cartertemm (at) gmail (dot) com.

Спасибо за поддержку!
